---
title: "Towards Fast Multilingual LLM Inference: Speculative Decoding and Specialized Drafters"
collection: publications
category: "conferences"
permalink: /publication/2024-11-12-Multilingual-Inference
date: 2024-11-12
venue: 'Empirical Methods for Natural Language Processing (EMNLP)'
paperurl: 'https://arxiv.org/abs/2406.16758'
citation: '<b>Euiin Yi</b>*, T. Kim*, H. Jeung, DS Chang, and S-Y. Yun. (2024). &quot;Towards Fast Multilingual LLM Inference: Speculative Decoding and Specialized Drafters.&quot; <i>Empirical Methods for Natural Language Processing (EMNLP)</i>.'
excerpt: '<img src="/assets/paper_images/MultiSpec.png" alt="placeholder image" style="float: right; width: 300px; margin: 0 0 1em 1em;">
This paper addresses the high inference cost of deploying LLMs in diverse language environments. It proposes a speculative decoding method using small, language-specialized drafter models. By employing language-specific drafters optimized through pre-training and fine-tuning, this approach demonstrates a significant improvement in LLM inference speed in multilingual contexts compared to existing methods.'
keywords: 'Multilingual LLMs, Inference Efficiency, Speculative Decoding, Drafter Models, Natural Language Processing'
---