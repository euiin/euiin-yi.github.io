---
title: "Revisiting Multi-Agent Debate as Test-Time Scaling: A Systematic Study of Conditional Effectiveness"
collection: publications
category: "workshops"
permalink: /publication/2025-01-01-Multi-Agent-Debate
date: 2025-01-01
venue: 'ICML Workshop MAS'
paperurl: 'https://arxiv.org/abs/2505.22960'
citation: 'Y Yang*, <b>Euiin Yi</b>*, J Ko, K Lee, and Z Jin. (2025). &quot;Revisiting Multi-Agent Debate as Test-Time Scaling: A Systematic Study of Conditional Effectiveness.&quot; <i>ICML MAS</i>.'
excerpt: '<img src="/assets/paper_images/Multiagent.png" alt="placeholder image" style="float: right; width: 300px; margin: 0 0 1em 1em;">
This study analyzes the effectiveness of Multi-Agent Debate (MAD) systems, where multiple LLM agents collaboratively solve problems through discussion. The research conceptualizes MAD as a test-time scaling technique and compares its performance against single agents in tasks like mathematical reasoning and safety. Findings indicate that MAD is more effective for more difficult problems or less capable models, and that agent diversity is crucial for safety-related tasks.'
keywords: 'Multi-Agent Systems, Large Language Models (LLMs), Collaborative Reasoning, Test-Time Scaling, AI Safety'
---